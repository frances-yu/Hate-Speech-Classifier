{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNLPVectors(filename):\n",
    "    file = 'nlp_data/' + filename + '.npy'\n",
    "    return np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabels():\n",
    "    return loadNLPVectors(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def genData(nlp):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(nlp, labels,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        shuffle = True,\n",
    "                                                        stratify = labels)\n",
    "    \n",
    "    train = [X_train, y_train]\n",
    "    test = [X_test, y_test]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_array = \"feature_array_unigram\"\n",
    "bigram_array = \"feature_array_bigram\"\n",
    "tfidf_array = \"feature_array_tfidf\"\n",
    "wordvec_array = \"feature_array_word2vec\"\n",
    "unigram_reduced = \"reduced_unigram\"\n",
    "bigram_reduced = \"reduced_bigram\"\n",
    "tfidf_reduced = \"reduced_tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = loadNLPVectors(unigram_array)\n",
    "bigram = loadNLPVectors(bigram_array)\n",
    "tfidf = loadNLPVectors(tfidf_array)\n",
    "word2vec = loadNLPVectors(wordvec_array)\n",
    "reduced_unigram = loadNLPVectors(unigram_reduced)\n",
    "reduced_bigram = loadNLPVectors(bigram_reduced)\n",
    "reduced_tfidf = loadNLPVectors(tfidf_reduced)\n",
    "labels = loadLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uni, test_uni = genData(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_big, test_big = genData(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf, test_tfidf = genData(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec, test_vec = genData(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_runi, test_runi = genData(reduced_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rbig, test_rbig = genData(reduced_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rtfidf, test_rtfidf = genData(reduced_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(hist, epoch_range):\n",
    "    \n",
    "    validation_loss = hist.history['val_loss']\n",
    "    validation_acc = hist.history['val_accuracy']\n",
    "    \n",
    "    epochs = range(1, epoch_range + 1)\n",
    "    \n",
    "    f = plt.figure(1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(epochs, validation_loss, 'bo')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    \n",
    "    g = plt.figure(2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(epochs, validation_acc, 'ro')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, history, test, epoch_range, nn_name, model_name):\n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    \n",
    "    pred_sigmoid = model.predict(X_test)\n",
    "    pred = []\n",
    "    for p in pred_sigmoid:\n",
    "        if p < .5:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    y_pred = np.asarray(pred)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    f = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(nn_name + ' ' + model_name + ' Evaluation: ')\n",
    "    print('Accuracy:       ', acc)\n",
    "    print('ROC AUC Score:  ', roc_auc)\n",
    "    print('F1 Score:       ', f)\n",
    "    print('Precision:      ', precision)\n",
    "    print('Recall:         ', recall)\n",
    "    \n",
    "    plotLoss(history, epoch_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(train, test, iterations = 10):\n",
    "    X_train = train[0]\n",
    "    y_train = train[1]\n",
    "    \n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    \n",
    "    #~13000, ~80000, 500, 200, 50\n",
    "    dim = len(X_train[0])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    if dim > 15000:\n",
    "        model.add(layers.Dense(500, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(64, activation = 'relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim > 10000:\n",
    "        model.add(layers.Dense(200, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(32, activation = 'relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim == 500:\n",
    "        model.add(layers.Dense(64, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(16, activation = 'relu'))\n",
    "    elif dim == 200:\n",
    "        model.add(layers.Dense(32, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim == 50:\n",
    "        model.add(layers.Dense(16, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu', input_shape = (dim, )))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs = iterations,\n",
    "                        batch_size = 128,\n",
    "                        validation_data = (X_test, y_test))\n",
    "    end = time.time() - start\n",
    "    \n",
    "    test = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"Training Time:  \", end)\n",
    "    print(\"Model Loss:     \", test[0])\n",
    "    print(\"Model Accuracy: \", test[1])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_runi = 50\n",
    "dnn_runi, dnn_runi_history = DNN(train_runi, test_runi, epoch_runi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rbig = 20\n",
    "dnn_rbig, dnn_rbig_history = DNN(train_rbig, test_rbig, epoch_rbig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rtfidf = 50\n",
    "dnn_rtfidf, dnn_rtfidf_history = DNN(train_rtfidf, test_rtfidf, epoch_rtfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vec = 50\n",
    "dnn_vec, dnn_vec_history = DNN(train_vec, test_vec, epoch_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_uni = 50\n",
    "dnn_uni, dnn_uni_history = DNN(train_uni, test_uni, epoch_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_big = 50\n",
    "dnn_big, dnn_big_history = DNN(train_big, test_big, epoch_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_tfidf = 50\n",
    "dnn_tfidf, dnn_tfidf_history = DNN(train_tfidf, test_tfidf, epoch_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_uni, dnn_uni_history, test_uni, epoch_uni, 'DNN', 'Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_big, dnn_big_history, test_big, epoch_big, 'DNN', 'Bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_tfidf, dnn_tfidf_history, test_tfidf, epoch_tfidf, 'DNN', 'TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_runi, dnn_runi_history, test_runi, epoch_runi, 'DNN', 'Reduced Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_rbig, dnn_rbig_history, test_rbig, epoch_rbig, 'DNN', 'Reduced Bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_rtfidf, dnn_rtfidf_history, test_rtfidf, epoch_rtfidf, 'DNN', 'Reduced TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_vec, dnn_vec_history, test_vec, epoch_vec, 'DNN', 'Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Reduced Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Reduced Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Reduced TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Reduced Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Reduced Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Reduced TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN TFIDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
