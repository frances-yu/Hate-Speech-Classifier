{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCSV(filename):\n",
    "    file = filename\n",
    "    if '.csv' not in filename:\n",
    "        file += '.csv'\n",
    "    data = pd.read_csv(file, encoding = 'ISO-8859-1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNLPVectors(filename):\n",
    "    file = 'nlp_data/' + filename + '.npy'\n",
    "    return np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabels():\n",
    "    return loadNLPVectors(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def genData(nlp):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(nlp, labels,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        shuffle = True,\n",
    "                                                        stratify = labels)\n",
    "    \n",
    "    train = [X_train, y_train]\n",
    "    test = [X_test, y_test]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_array = \"feature_array_unigram\"\n",
    "bigram_array = \"feature_array_bigram\"\n",
    "tfidf_array = \"feature_array_tfidf\"\n",
    "wordvec_array = \"feature_array_word2vec\"\n",
    "unigram_reduced = \"reduced_unigram\"\n",
    "bigram_reduced = \"reduced_bigram\"\n",
    "tfidf_reduced = \"reduced_tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = loadNLPVectors(unigram_array)\n",
    "bigram = loadNLPVectors(bigram_array)\n",
    "tfidf = loadNLPVectors(tfidf_array)\n",
    "word2vec = loadNLPVectors(wordvec_array)\n",
    "reduced_unigram = loadNLPVectors(unigram_reduced)\n",
    "reduced_bigram = loadNLPVectors(bigram_reduced)\n",
    "reduced_tfidf = loadNLPVectors(tfidf_reduced)\n",
    "labels = loadLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uni, test_uni = genData(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_big, test_big = genData(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf, test_tfidf = genData(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec, test_vec = genData(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_runi, test_runi = genData(reduced_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rbig, test_rbig = genData(reduced_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rtfidf, test_rtfidf = genData(reduced_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFigures(hist, epoch_range):\n",
    "    \n",
    "    validation_loss = hist.history['val_loss']\n",
    "    validation_acc = hist.history['val_accuracy']\n",
    "    \n",
    "    epochs = range(1, epoch_range + 1)\n",
    "    \n",
    "    f = plt.figure(1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(epochs, validation_loss, 'bo')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    \n",
    "    g = plt.figure(2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(epochs, validation_acc, 'ro')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, history, test, name):\n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    epoch_range = len(history.history['loss'])\n",
    "    \n",
    "    pred_sigmoid = model.predict(X_test)\n",
    "    pred = []\n",
    "    for p in pred_sigmoid:\n",
    "        if p < .5:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    y_pred = np.asarray(pred)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    f = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(name + ' Evaluation: ')\n",
    "    print('Accuracy:       ', acc)\n",
    "    print('ROC AUC Score:  ', roc_auc)\n",
    "    print('F1 Score:       ', f)\n",
    "    print('Precision:      ', precision)\n",
    "    print('Recall:         ', recall)\n",
    "    \n",
    "    plotFigures(history, epoch_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(train, test, iterations = 50, early_stop = False):\n",
    "    X_train = train[0]\n",
    "    y_train = train[1]\n",
    "    \n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    \n",
    "    dim = len(X_train[0])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    if dim > 15000:\n",
    "        model.add(layers.Dense(500, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(64, activation = 'relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim > 10000:\n",
    "        model.add(layers.Dense(200, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(32, activation = 'relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim == 500:\n",
    "        model.add(layers.Dense(64, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(16, activation = 'relu'))\n",
    "    elif dim == 200:\n",
    "        model.add(layers.Dense(32, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu'))\n",
    "    elif dim == 50:\n",
    "        model.add(layers.Dense(16, activation = 'relu', input_shape = (dim,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(4, activation = 'relu', input_shape = (dim, )))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    callback = [EarlyStopping(monitor = 'val_loss', min_delta = .0001)]\n",
    "    if early_stop:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            verbose = 0,\n",
    "                            batch_size = 128,\n",
    "                            callbacks = callback,\n",
    "                            validation_data = (X_test, y_test))\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            verbose = 0,\n",
    "                            batch_size = 128,\n",
    "                            callbacks = callback,\n",
    "                            validation_data = (X_test, y_test))\n",
    "        \n",
    "    end = time.time() - start\n",
    "    \n",
    "    test_vals = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"Training Time:  \", end)\n",
    "    print(\"Model Loss:     \", test_vals[0])\n",
    "    print(\"Model Accuracy: \", test_vals[1])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tokenized Data for CNN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenData():\n",
    "    tokenizer = Tokenizer()\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    data = loadCSV(\"binary_classification\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[\"tweet_text\"], data[\"tweet_class\"],\n",
    "                                                        test_size = 0.2, random_state = 42,\n",
    "                                                        shuffle = True, \n",
    "                                                        stratify = data[\"tweet_class\"])\n",
    "    \n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    max_train = max([len(x) for x in train_seq])\n",
    "    max_test = max([len(x) for x in test_seq])\n",
    "    max_len = max([max_train, max_test])\n",
    "    \n",
    "    train_seq_array = pad_sequences(train_seq, maxlen = max_len)\n",
    "    test_seq_array = pad_sequences(test_seq, maxlen = max_len)\n",
    "    \n",
    "    train_label = encoder.fit_transform(y_train)\n",
    "    test_label = encoder.fit_transform(y_test)\n",
    "    \n",
    "    train = [train_seq_array, train_label]\n",
    "    test = [test_seq_array, test_label]\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    return train, test, max_len, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(iterations = 50, early_stop = False):\n",
    "    train, test, max_len, vocab_size = tokenData()\n",
    "\n",
    "    X_train = train[0]\n",
    "    y_train = train[1]\n",
    "    \n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(input_dim = vocab_size, output_dim = 32, input_length = max_len))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv1D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size = 2))\n",
    "    model.add(layers.Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    callback = [EarlyStopping(monitor = 'val_loss', min_delta = .0001)]\n",
    "    \n",
    "    start = time.time()\n",
    "    if early_stop:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            batch_size = 128,\n",
    "                            callbacks = callback,\n",
    "                            validation_data = (X_test, y_test))\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            batch_size = 128,\n",
    "                            validation_data = (X_test, y_test))\n",
    "    end = time.time() - start\n",
    "    \n",
    "    test_vals = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"Training Time:  \", end)\n",
    "    print(\"Model Loss:     \", test_vals[0])\n",
    "    print(\"Model Accuracy: \", test_vals[1])\n",
    "    \n",
    "    return model, history, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Recurrent Neural Network with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(iterations = 50, early_stop = False):\n",
    "    train, test, max_len, vocab_size = tokenData()\n",
    "\n",
    "    X_train = train[0]\n",
    "    y_train = train[1]\n",
    "    \n",
    "    X_test = test[0]\n",
    "    y_test = test[1]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(input_dim = vocab_size, output_dim = 128, input_len = max_len))\n",
    "    model.add(layers.SpatialDropout1D(0.5))\n",
    "    model.add(layers.LSTM(128, dropout = 0.5, recurrent_dropout = 0.5))\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    callback = [EarlyStopping(monitor = 'val_loss', min_delta = .0001)]\n",
    "    \n",
    "    start = time.time()\n",
    "    if early_stop:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            batch_size = 128,\n",
    "                            callbacks = callback,\n",
    "                            validation_data = (X_test, y_test))\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs = iterations,\n",
    "                            batch_size = 128,\n",
    "                            validation_data = (X_test, y_test))\n",
    "    end = time.time() - start\n",
    "    \n",
    "    test_vals = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"Training Time:  \", end)\n",
    "    print(\"Model Loss:     \", test_vals[0])\n",
    "    print(\"Model Accuracy: \", test_vals[1])\n",
    "    \n",
    "    return model, history, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_runi, dnn_runi_history = DNN(train_runi, test_runi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_rbig, dnn_rbig_history = DNN(train_rbig, test_rbig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_rtfidf, dnn_rtfidf_history = DNN(train_rtfidf, test_rtfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_vec, dnn_vec_history = DNN(train_vec, test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_uni, dnn_uni_history = DNN(train_uni, test_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_big, dnn_big_history = DNN(train_big, test_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_tfidf, dnn_tfidf_history = DNN(train_tfidf, test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_uni, dnn_uni_history, test_uni, 'DNN Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_big, dnn_big_history, test_big, 'DNN Bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_tfidf, dnn_tfidf_history, test_tfidf, 'DNN TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_runi, dnn_runi_history, test_runi, 'DNN Reduced Unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_rbig, dnn_rbig_history, test_rbig, 'DNN Reduced Bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_rtfidf, dnn_rtfidf_history, test_rtfidf, 'DNN Reduced TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dnn_vec, dnn_vec_history, test_vec, 'DNN Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model, cnn_model_history, test_cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cnn_model, cnn_model_history, test_cnn, 'Convolutional Neural Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model, rnn_model_history, test_rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rnn_model, rnn_model_history, test_rnn, 'Recurrent Neural Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks - Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Reduced TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Deep Neural Networks - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Convolutional Neural Network - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Recurrent Neural Network - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
