{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNLPVectors(filename):\n",
    "    file = 'nlp_data/' + filename + '.npy'\n",
    "    return np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLabels():\n",
    "    return loadNLPVectors(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestData(nlp):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(nlp, \n",
    "                                                        labels, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 42, \n",
    "                                                        shuffle = True, \n",
    "                                                        stratify = labels)\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_array = \"feature_array_unigram\"\n",
    "bigram_array = \"feature_array_bigram\"\n",
    "tfidf_array = \"feature_array_tfidf\"\n",
    "wordvec_array = \"feature_array_word2vec\"\n",
    "unigram_reduced = \"reduced_unigram\"\n",
    "bigram_reduced = \"reduced_bigram\"\n",
    "tfidf_reduced = \"reduced_tfidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = loadNLPVectors(unigram_array)\n",
    "bigram = loadNLPVectors(bigram_array)\n",
    "tfidf = loadNLPVectors(tfidf_array)\n",
    "word2vec = loadNLPVectors(wordvec_array)\n",
    "reduced_unigram = loadNLPVectors(unigram_reduced)\n",
    "reduced_bigram = loadNLPVectors(bigram_reduced)\n",
    "reduced_tfidf = loadNLPVectors(tfidf_reduced)\n",
    "labels = loadLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_uni, y_uni = createTestData(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_big, y_big = createTestData(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf, y_tfidf = createTestData(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec, y_vec = createTestData(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_runi, y_runi = createTestData(reduced_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rbig, y_rbig = createTestData(reduced_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rtfidf, y_rtfidf = createTestData(reduced_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_folders = ['unigram', 'bigram', 'tfidf', 'word2vec', 'reduced_unigram', 'reduced_bigram', 'reduced_tfidf']\n",
    "classifiers = ['rand_forest', 'log_reg', 'lin_reg', 'naive_bayes', 'svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(nlp_index, clf_index):\n",
    "    model_path = 'models/' + feature_folders[nlp_index] + '/' + classifiers[clf_index] + '.pkl'\n",
    "    model = joblib.load(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
