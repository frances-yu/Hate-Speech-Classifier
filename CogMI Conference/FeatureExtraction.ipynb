{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCSV(filename):\n",
    "    file = filename\n",
    "    if '.csv' not in filename:\n",
    "        file += '.csv'\n",
    "    data = pd.read_csv(file, encoding = 'ISO-8859-1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = loadCSV('processed_1')\n",
    "d2 = loadCSV('processed_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Scenarios\n",
    "\n",
    "### 1) Train and test classifiers on data from only dataset 1\n",
    "### 2) Train and test classifiers on data from only dataset 2\n",
    "### 3) Train classifiers on data from dataset 1 and test them on dataset 2\n",
    "### 4) Train classifiers on data from dataset 2 and test them on dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Tweets into Training and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(d1['tweet'].tolist(),\n",
    "                                                        d1['class'].tolist(),\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        shuffle = True,\n",
    "                                                        stratify = d1['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(d2['tweet'].tolist(),\n",
    "                                                        d2['class'].tolist(),\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        shuffle = True,\n",
    "                                                        stratify = d2['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train, X3_test = d1['tweet'].tolist(), d2['tweet'].tolist()\n",
    "y3_train, y3_test = d1['class'].tolist(), d2['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train, X4_test = d2['tweet'].tolist(), d1['tweet'].tolist()\n",
    "y4_train, y4_test = d2['class'].tolist(), d1['class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUnigram(train, test, total_features):\n",
    "    \n",
    "    train_unigram = CountVectorizer(max_features = total_features)\n",
    "    train_matrix = train_unigram.fit_transform(train)\n",
    "    \n",
    "    test_unigram = CountVectorizer(vocabulary = train_unigram.get_feature_names())\n",
    "    test_matrix = test_unigram.fit_transform(test)\n",
    "    \n",
    "    \n",
    "    return train_matrix.toarray(), test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUnigramTfidf(train, test, total_features):\n",
    "    train_tfidf = TfidfVectorizer(max_features = total_features)\n",
    "    train_matrix = train_tfidf.fit_transform(train)\n",
    "    \n",
    "    test_tfidf = TfidfVectorizer(vocabulary = train_tfidf.get_feature_names())\n",
    "    test_matrix = test_tfidf.fit_transform(test)\n",
    "    \n",
    "    return train_matrix.toarray(), test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBigram(train, test, total_features):\n",
    "    train_bigram = CountVectorizer(ngram_range = (2,2), max_features = total_features)\n",
    "    train_matrix = train_bigram.fit_transform(train)\n",
    "    \n",
    "    test_bigram = CountVectorizer(ngram_range = (2,2), vocabulary = train_bigram.get_feature_names())\n",
    "    test_matrix = test_bigram.fit_transform(test)\n",
    "    \n",
    "    return train_matrix.toarray(), test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBigramTfidf(train, test, total_features):\n",
    "    train_tfidf = TfidfVectorizer(ngram_range = (2,2), max_features = total_features)\n",
    "    train_matrix = train_tfidf.fit_transform(train)\n",
    "    \n",
    "    test_tfidf = TfidfVectorizer(ngram_range = (2,2), vocabulary = train_tfidf.get_feature_names())\n",
    "    test_matrix = test_tfidf.fit_transform(test)\n",
    "    \n",
    "    return train_matrix.toarray(), test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDim(train, test):\n",
    "    n = int(len(train[0])/20)\n",
    "    tsvd = TruncatedSVD(n_components = n)\n",
    "    \n",
    "    rtrain = tsvd.fit_transform(train)\n",
    "    rtest = tsvd.fit_transform(test)\n",
    "    \n",
    "    return rtrain, rtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feautures from Unigram/TF-IDF Unigram:\n",
    "\n",
    "Scenario 1) 11671 reduced to 9500\n",
    "\n",
    "Scenario 2) 15000 reduced to 12000\n",
    "\n",
    "Scenario 3) 13380 reduced to 11000\n",
    "\n",
    "Scenario 4) 17704 reduced to 14000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feautures from Bigram/TF-IDF Bigram:\n",
    "\n",
    "Scenario 1) 54205 reduced to 43000\n",
    "\n",
    "Scenario 2) 88523 reduced to 71000\n",
    "\n",
    "Scenario 3) 66368 reduced to 53000\n",
    "\n",
    "Scenario 4) 107099 reduced to 86000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionally Reduced Unigram/TF-IDF Unigram:\n",
    "\n",
    "Scenario 1) 9500 reduced to 475\n",
    "\n",
    "Scenario 2) 12000 reduced to 600\n",
    "\n",
    "Scenario 3) 11000 reduced to 550\n",
    "\n",
    "Scenario 4) 14000 reduced to 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionally Reduced Bigram/TF-IDF Bigram:\n",
    "\n",
    "Scenario 1) 43000 reduced to 2150\n",
    "\n",
    "Scenario 2) 71000 reduced to 3550\n",
    "\n",
    "Scenario 3) 53000 reduced to 2650\n",
    "\n",
    "Scenario 4) 86000 reduced to 4300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Save Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_1')\n",
    "os.mkdir('nlp/scenario_2')\n",
    "os.mkdir('nlp/scenario_3')\n",
    "os.mkdir('nlp/scenario_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_1/labels')\n",
    "os.mkdir('nlp/scenario_2/labels')\n",
    "os.mkdir('nlp/scenario_3/labels')\n",
    "os.mkdir('nlp/scenario_4/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_1/unigram')\n",
    "os.mkdir('nlp/scenario_1/unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_1/bigram')\n",
    "os.mkdir('nlp/scenario_1/bigram_tfidf')\n",
    "os.mkdir('nlp/scenario_1/reduced_unigram')\n",
    "os.mkdir('nlp/scenario_1/reduced_unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_1/reduced_bigram')\n",
    "os.mkdir('nlp/scenario_1/reduced_bigram_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_2/unigram')\n",
    "os.mkdir('nlp/scenario_2/unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_2/bigram')\n",
    "os.mkdir('nlp/scenario_2/bigram_tfidf')\n",
    "os.mkdir('nlp/scenario_2/reduced_unigram')\n",
    "os.mkdir('nlp/scenario_2/reduced_unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_2/reduced_bigram')\n",
    "os.mkdir('nlp/scenario_2/reduced_bigram_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_3/unigram')\n",
    "os.mkdir('nlp/scenario_3/unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_3/bigram')\n",
    "os.mkdir('nlp/scenario_3/bigram_tfidf')\n",
    "os.mkdir('nlp/scenario_3/reduced_unigram')\n",
    "os.mkdir('nlp/scenario_3/reduced_unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_3/reduced_bigram')\n",
    "os.mkdir('nlp/scenario_3/reduced_bigram_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('nlp/scenario_4/unigram')\n",
    "os.mkdir('nlp/scenario_4/unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_4/bigram')\n",
    "os.mkdir('nlp/scenario_4/bigram_tfidf')\n",
    "os.mkdir('nlp/scenario_4/reduced_unigram')\n",
    "os.mkdir('nlp/scenario_4/reduced_unigram_tfidf')\n",
    "os.mkdir('nlp/scenario_4/reduced_bigram')\n",
    "os.mkdir('nlp/scenario_4/reduced_bigram_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(scenario, nlp, name, data):\n",
    "    filepath = 'nlp/scenario_%s/%s/%s.npy' % (scenario, nlp, name)\n",
    "    np.save(filepath, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'labels', 'train', y1_train)\n",
    "saveFile(1, 'labels', 'test', y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'labels', 'train', y2_train)\n",
    "saveFile(2, 'labels', 'test', y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'labels', 'train', y3_train)\n",
    "saveFile(3, 'labels', 'test', y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'labels', 'train', y4_train)\n",
    "saveFile(4, 'labels', 'test', y4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_uni, test1_uni = makeUnigram(X1_train, X1_test, 9500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_uni, test2_uni = makeUnigram(X2_train, X2_test, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_uni, test3_uni = makeUnigram(X3_train, X3_test, 11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_uni, test4_uni = makeUnigram(X4_train, X4_test, 14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'unigram', 'train', train1_uni)\n",
    "saveFile(1, 'unigram', 'test', test1_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'unigram', 'train', train2_uni)\n",
    "saveFile(2, 'unigram', 'test', test2_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'unigram', 'train', train3_uni)\n",
    "saveFile(3, 'unigram', 'test', test3_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'unigram', 'train', train4_uni)\n",
    "saveFile(4, 'unigram', 'test', test4_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Unigram - Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_tfu, test1_tfu = makeUnigramTfidf(X1_train, X1_test, 9500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_tfu, test2_tfu = makeUnigramTfidf(X2_train, X2_test, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_tfu, test3_tfu = makeUnigramTfidf(X3_train, X3_test, 11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_tfu, test4_tfu = makeUnigramTfidf(X4_train, X4_test, 14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'unigram_tfidf', 'train', train1_tfu)\n",
    "saveFile(1, 'unigram_tfidf', 'test', test1_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'unigram_tfidf', 'train', train2_tfu)\n",
    "saveFile(2, 'unigram_tfidf', 'test', test2_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'unigram_tfidf', 'train', train3_tfu)\n",
    "saveFile(3, 'unigram_tfidf', 'test', test3_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'unigram_tfidf', 'train', train4_tfu)\n",
    "saveFile(4, 'unigram_tfidf', 'test', test4_tfu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_big, test1_big = makeBigram(X1_train, X1_test, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_big, test2_big = makeBigram(X2_train, X2_test, 71000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_big, test3_big = makeBigram(X3_train, X3_test, 53000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_big, test4_big = makeBigram(X4_train, X4_test, 86000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'bigram', 'train', train1_big)\n",
    "saveFile(1, 'bigram', 'test', test1_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'bigram', 'train', train2_big)\n",
    "saveFile(2, 'bigram', 'test', test2_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'bigram', 'train', train3_big)\n",
    "saveFile(3, 'bigram', 'test', test3_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'bigram', 'train', train4_big)\n",
    "saveFile(4, 'bigram', 'test', test4_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Bigram - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_tfb, test1_tfb = makeBigramTfidf(X1_train, X1_test, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_tfb, test2_tfb = makeBigramTfidf(X2_train, X2_test, 71000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_tfb, test3_tfb = makeBigramTfidf(X3_train, X3_test, 53000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_tfb, test4_tfb = makeBigramTfidf(X4_train, X4_test, 86000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'bigram_tfidf', 'train', train1_tfb)\n",
    "saveFile(1, 'bigram_tfidf', 'test', test1_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'bigram_tfidf', 'train', train2_tfb)\n",
    "saveFile(2, 'bigram_tfidf', 'test', test2_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'bigram_tfidf', 'train', train3_tfb)\n",
    "saveFile(3, 'bigram_tfidf', 'test', test3_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'bigram_tfidf', 'train', train4_tfb)\n",
    "saveFile(4, 'bigram_tfidf', 'test', test4_tfb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_runi, test1_runi = reduceDim(train1_uni, test1_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_runi, test2_runi = reduceDim(train2_uni, test2_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_runi, test3_runi = reduceDim(train3_uni, test3_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_runi, test4_runi = reduceDim(train4_uni, test4_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'reduced_unigram', 'train', train1_runi)\n",
    "saveFile(1, 'reduced_unigram', 'test', test1_runi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'reduced_unigram', 'train', train2_runi)\n",
    "saveFile(2, 'reduced_unigram', 'test', test2_runi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'reduced_unigram', 'train', train3_runi)\n",
    "saveFile(3, 'reduced_unigram', 'test', test3_runi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'reduced_unigram', 'train', train4_runi)\n",
    "saveFile(4, 'reduced_unigram', 'test', test4_runi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Unigram - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_rtfu, test1_rtfu = reduceDim(train1_tfu, test1_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_rtfu, test2_rtfu = reduceDim(train2_tfu, test2_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_rtfu, test3_rtfu = reduceDim(train3_tfu, test3_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_rtfu, test4_rtfu = reduceDim(train4_tfu, test4_tfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'reduced_unigram_tfidf', 'train', train1_rtfu)\n",
    "saveFile(1, 'reduced_unigram_tfidf', 'test', test1_rtfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'reduced_unigram_tfidf', 'train', train2_rtfu)\n",
    "saveFile(2, 'reduced_unigram_tfidf', 'test', test2_rtfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'reduced_unigram_tfidf', 'train', train3_rtfu)\n",
    "saveFile(3, 'reduced_unigram_tfidf', 'test', test3_rtfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'reduced_unigram_tfidf', 'train', train4_rtfu)\n",
    "saveFile(4, 'reduced_unigram_tfidf', 'test', test4_rtfu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_rbig, test1_rbig = reduceDim(train1_big, test1_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_rbig, test2_rbig = reduceDim(train2_big, test2_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_rbig, test3_rbig = reduceDim(train3_big, test3_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_rbig, test4_rbig = reduceDim(train4_big, test4_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'reduced_bigram', 'train', train1_rbig)\n",
    "saveFile(1, 'reduced_bigram', 'test', test1_rbig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'reduced_bigram', 'train', train2_rbig)\n",
    "saveFile(2, 'reduced_bigram', 'test', test2_rbig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'reduced_bigram', 'train', train3_rbig)\n",
    "saveFile(3, 'reduced_bigram', 'test', test3_rbig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'reduced_bigram', 'train', train4_rbig)\n",
    "saveFile(4, 'reduced_bigram', 'test', test4_rbig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Bigram - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_rtfb, test1_rtfb = reduceDim(train1_tfb, test1_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_rtfb, test2_rtfb = reduceDim(train2_tfb, test2_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_rtfb, test3_rtfb = reduceDim(train3_tfb, test3_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_rtfb, test4_rtfb = reduceDim(train4_tfb, test4_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(1, 'reduced_bigram_tfidf', 'train', train1_rtfb)\n",
    "saveFile(1, 'reduced_bigram_tfidf', 'test', test1_rtfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(2, 'reduced_bigram_tfidf', 'train', train2_rtfb)\n",
    "saveFile(2, 'reduced_bigram_tfidf', 'test', test2_rtfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(3, 'reduced_bigram_tfidf', 'train', train3_rtfb)\n",
    "saveFile(3, 'reduced_bigram_tfidf', 'test', test3_rtfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(4, 'reduced_bigram_tfidf', 'train', train4_rtfb)\n",
    "saveFile(4, 'reduced_bigram_tfidf', 'test', test4_rtfb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
